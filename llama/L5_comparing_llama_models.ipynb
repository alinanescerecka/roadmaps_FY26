{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde90571-0799-48af-b315-247f85c2269b",
   "metadata": {},
   "source": [
    "# Comparing Llama Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce53734-47df-454d-bb29-c135b2a1b338",
   "metadata": {},
   "source": [
    "**Update: Llama 3 was released on April 18 and this notebook has been updated to compare Llama 3 and Llama 2 models hosted on Together.ai.**\n",
    "\n",
    "- Load helper function to prompt Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469f76b7-acd9-4192-93f2-a37d4ac9d196",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import llama, llama_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a619c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-llama/Meta-Llama-3-8B-Instruct-Lite\n",
    "# meta-llama/Llama-3-70b-chat-hf\n",
    "# meta-llama/Meta-Llama-3-70B-Instruct-Turbo\n",
    "\n",
    "# meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
    "# meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd039f-d768-44a7-b81b-bfab14a33a90",
   "metadata": {},
   "source": [
    "### Task 1: Sentiment Classification\n",
    "- Compare the models on few-shot prompt sentiment classification.\n",
    "- You are asking the model to return a one word response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d44286-dd86-48bb-9457-72dca64b5b37",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: Positive\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "Message: Can't wait to order pizza for dinner tonight!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38108527-032d-411b-bb3e-31471694758f",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) to get the response.\n",
    "\n",
    "**Note the model names accepted by Together.ai are case insensitive and can be either \"META-LLAMA/LLAMA-2-7B-CHAT-HF\" or \"togethercomputer/llama-2-7b-chat\". The names starting with \"META-LLAMA\" are preferred now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58a65f2-a8e2-4cf4-abc4-c65fd64710c9",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give a one word response.\n",
      "[/INST]  Sentiment: Positive\n",
      "```\n",
      "\n",
      "**INST**  Sentiment: Positive\n",
      "```\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "Message: Can't wait to order pizza for dinner tonight!\n",
      "Sentiment: ?\n",
      "\n",
      "Give\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 #model=\"togethercomputer/LLama-2-7b-chat\")\n",
    "                 model = \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24729f9d-0899-472b-9025-12824b962a52",
   "metadata": {},
   "source": [
    "- Now, use the 70B parameter chat model (`llama-2-70b-chat`) on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f62ef43-0b6a-4c11-bed9-8f49ec4f0802",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 #model=\"togethercomputer/llama-2-70b-chat\")\n",
    "                 model=\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4a50d-d872-4707-996b-885a2e310915",
   "metadata": {},
   "source": [
    "**Using Llama 3 chat models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc8e6e8-986e-47e6-8691-1f0aabcea1ea",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKDE3t-2kFHot-9ae6547efd94e4e9-SEA', 'object': 'text_completion', 'created': 1765805788, 'model': 'meta-llama/Llama-4-Scout-17B-16E-Instruct', 'choices': [{'index': 0, 'text': '\\n\\nPositive.', 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 66, 'total_tokens': 70, 'completion_tokens': 4, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2842f4-5c9c-4796-a012-1e4a5a7c205c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKDMYL-2kFHot-9ae6551c7cfbe4d7-SEA', 'object': 'text_completion', 'created': 1765805813, 'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', 'choices': [{'index': 0, 'text': ' Positive. [/INST] \\n\\nPositive.', 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 66, 'total_tokens': 75, 'completion_tokens': 9, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c5ded-3bbe-4cac-898c-578655ea47cc",
   "metadata": {},
   "source": [
    "### Task 2: Summarization\n",
    "- Compare the models on summarization task.\n",
    "- This is the same \"email\" as the one you used previously in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07d248f0-a610-4696-aacd-12ca94b2fb8c",
   "metadata": {
    "height": 572
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task â€” the input and the desired output â€” sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If youâ€™re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesnâ€™t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesnâ€™t deliver the performance you want, then try fine-tuning â€” but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? ðŸ˜œ)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, thatâ€™s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLMâ€™s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning â€” in which GPT-4 surpasses current open models â€” it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, itâ€™s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. Iâ€™ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "\n",
    "What did the author say about llama models?\n",
    "```\n",
    "{email}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f00bb-51d9-492a-920b-5b286ab5f5c4",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c60da6-9b3e-4f11-a1d5-8e5e273e91ac",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summarize this email and extract some key points.\n",
      "\n",
      "The author, Andrew, discusses the various ways to build applications using large language models (LLMs). He highlights the following key points:\n",
      "\n",
      "* LLMs with permissive licenses offer more options for developers.\n",
      "* There are different ways to build applications, ranging from prompting to fine-tuning, with varying levels of complexity and cost.\n",
      "* Prompting is a good starting point, as it allows for quick prototyping without requiring a large training set.\n",
      "* One-shot or few-shot prompting can lead to better results by providing a few examples of the desired output.\n",
      "* Fine-tuning requires more resources and is more complex, but can lead to better results.\n",
      "* Pretraining is the most complex and resource-intensive approach, but can lead to specialized models.\n",
      "\n",
      "The author also mentions the following:\n",
      "\n",
      "* The course \"Generative AI with Large Language Models\" is recommended for a deeper understanding of the options.\n",
      "* Fine-tuning a proprietary model, such as GPT-4, can be challenging and may not yield superior results.\n",
      "* Choosing the right model is important, as smaller models may be sufficient for many applications, while larger models may have more knowledge and better reasoning ability.\n",
      "\n",
      "The author's tone is informative and encouraging, with a dash of humor, as seen in the fun fact about a team member trying to fine-tune Llama-2-7B to sound like him. Overall, the email aims to educate and guide developers in their use of LLMs. [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]\n"
     ]
    }
   ],
   "source": [
    "response_7b = llama(prompt,\n",
    "                model=\"meta-llama/Meta-Llama-3-8B-Instruct-Lite\")\n",
    "print(response_7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84506c-c6b9-4512-9fbc-1e27140d4a2f",
   "metadata": {},
   "source": [
    "- Now, use the 13B parameter chat model (`llama-2-13b-chat`) to summarize the email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa0c61-ff78-4372-b4fb-a51be7bf11eb",
   "metadata": {},
   "source": [
    "- Lastly, use the 70B parameter chat model (`llama-2-70b-chat`) to summarize the email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925c56a-7fb9-46ba-9680-8dc489cc0ce7",
   "metadata": {},
   "source": [
    "**Using Llama 4 chat models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17331039-712d-444d-b0c8-1dac330cd6c1",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKEErY-2kFHot-9ae659534e27a879-RIX', 'object': 'text_completion', 'created': 1765805989, 'model': 'meta-llama/Llama-4-Scout-17B-16E-Instruct', 'choices': [{'index': 0, 'text': '\\n\\n**Email Summary**\\n\\nThe email discusses the increasing availability of open-source large language models (LLMs) and the various ways to build applications using these models, ranging from simple prompting to more complex fine-tuning and pretraining. The author, Andrew, provides recommendations for developers on how to approach building LLM-based applications, including starting with prompting and gradually moving to more complex techniques if needed.\\n\\n**Key Points**\\n\\n* There are several ways to build applications using LLMs, including:\\n\\t+ Prompting\\n\\t+ One-shot or few-shot prompting\\n\\t+ Fine-tuning\\n\\t+ Pretraining\\n* The choice of approach depends on the desired level of complexity and the quality of output required\\n* Fine-tuning a smaller model may not always yield superior results to prompting a larger, more capable model\\n* Choosing a specific model depends on factors such as processing power, knowledge, and reasoning ability\\n\\n**Llama Models**\\n\\nThe author mentions Llama-2-7B in a humorous note, stating that a member of the DeepLearning.AI team has been trying to fine-tune it to sound like him, jokingly wondering if his job is at risk ðŸ˜œ. No specific technical details about Llama models are provided in the email.', 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 688, 'total_tokens': 937, 'completion_tokens': 249, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b = llama(prompt,\n",
    "                model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "print(response_llama3_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac179a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n**Email Summary**\\n\\nThe email discusses the increasing availability of open-source large language models (LLMs) and the various ways to build applications using these models, ranging from simple prompting to more complex fine-tuning and pretraining. The author, Andrew, provides recommendations for developers on how to approach building LLM-based applications, including starting with prompting and gradually moving to more complex techniques if needed.\\n\\n**Key Points**\\n\\n* There are several ways to build applications using LLMs, including:\\n\\t+ Prompting\\n\\t+ One-shot or few-shot prompting\\n\\t+ Fine-tuning\\n\\t+ Pretraining\\n* The choice of approach depends on the desired level of complexity and the quality of output required\\n* Fine-tuning a smaller model may not always yield superior results to prompting a larger, more capable model\\n* Choosing a specific model depends on factors such as processing power, knowledge, and reasoning ability\\n\\n**Llama Models**\\n\\nThe author mentions Llama-2-7B in a humorous note, stating that a member of the DeepLearning.AI team has been trying to fine-tune it to sound like him, jokingly wondering if his job is at risk ðŸ˜œ. No specific technical details about Llama models are provided in the email.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_llama3_8b['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c7aa9e3-819d-471b-a776-e5e37265e326",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKESx1-2kFHot-9ae65a514ad9042f-SEA', 'object': 'text_completion', 'created': 1765806031, 'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', 'choices': [{'index': 0, 'text': ' Here is a summary of the email and some key points:\\n\\n**Summary:** The email discusses the increasing availability of open-source large language models (LLMs) and various ways to build applications based on them, ranging from simple prompting to more complex fine-tuning and pretraining. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\\n\\n**Key Points:**\\n\\n1. **Building applications with LLMs:** The author lists four ways to build applications with LLMs, in increasing order of cost/complexity: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\\n2. **Recommended approach:** Start with prompting, and if the output quality is unsatisfactory, try one-shot or few-shot prompting, then RAG (retrieval augmented generation), and finally fine-tuning if necessary.\\n3. **Fine-tuning vs. prompting:** Fine-tuning a smaller model may yield superior results than prompting a larger model if the goal is to change the output style, but may not be sufficient for complex reasoning tasks.\\n4. **Model selection:** The author notes that smaller models require less processing power, while larger models have more knowledge and better reasoning ability, and promises to discuss model selection in a future letter.\\n5. **Llama models:** The author mentions that a member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like them, but does not provide any further commentary or evaluation of the Llama models themselves. The mention is made in a humorous tone, suggesting that the author is amused by the attempt rather than providing a serious assessment. \\n\\nOverall, the email provides guidance on building applications with LLMs and highlights the trade-offs between different approaches and model sizes. The mention of Llama models is a lighthearted aside rather than a serious discussion. \\n\\nIf you would like me to clarify or expand on any of these points, please let me know.', 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 688, 'total_tokens': 1078, 'completion_tokens': 390, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b = llama(prompt,\n",
    "                model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\")\n",
    "print(response_llama3_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f051d-a7e4-45cc-b162-646862084994",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Summarization\n",
    "\n",
    "- Interestingly, you can ask a LLM to evaluate the responses of other LLMs.\n",
    "- This is known as **Model-Graded Evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77de7b-7339-4ff3-b8fd-63cb448d4ed8",
   "metadata": {},
   "source": [
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`llama-2-70b-chat`).\n",
    "- In the `prompt`, provide the \"email\", \"name of the models\", and the \"summary\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10e1d72-10ab-43dd-80f8-74fcf57e28a4",
   "metadata": {
    "height": 591
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKG1jd-2kFHot-9ae661c6580e74f5-RIX', 'object': 'text_completion', 'created': 1765806338, 'model': 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8', 'choices': [{'index': 0, 'text': \" Evaluation of the summaries generated by the models:\\n\\n### llama-3\\n\\n* **Does it summarize the original text well?**: Yes, the summary captures the main points of the email, including the different ways to build applications using LLMs and the author's recommendations.\\n* **Does it follow the instructions of the prompt?**: Yes, the summary follows the prompt's instruction to summarize the email and extract key points.\\n* **Are there any other interesting characteristics of the model's output?**: The summary is well-structured and easy to follow. The model also captures the author's tone, including the humorous note about fine-tuning Llama-2-7B.\\n\\n### llama-4\\n\\n* **Does it summarize the original text well?**: Yes, the summary accurately conveys the main points of the email, including the different approaches to building LLM-based applications.\\n* **Does it follow the instructions of the prompt?**: The prompt is not explicitly stated, but the summary appears to be a good representation of the email's content.\\n* **Are there any other interesting characteristics of the model's output?**: The summary is concise and well-organized, using bullet points to highlight key information.\\n\\n### llama-4-maverick\\n\\n* **Does it summarize the original text well?**: Yes, the summary provides a clear and accurate representation of the email's content, including the author's recommendations and key points.\\n* **Does it follow the instructions of the prompt?**: The summary appears to follow the implicit prompt to summarize the email and extract key points.\\n* **Are there any other interesting characteristics of the model's output?**: The summary is detailed and provides a good analysis of the email's content. The model also captures the author's tone, including the humorous note about fine-tuning Llama-2-7B.\\n\\nComparison of the models:\\n\\nAll three models (llama-3, llama-4, and llama-4-maverick) perform well in summarizing the original text and capturing the key points. They also follow the implicit or explicit prompt to summarize the email.\\n\\nThe main differences between the models are:\\n\\n* **Level of detail**: llama-4-maverick provides the most detailed summary, while llama-4 is more concise.\\n* **Structure**: llama-3 and llama-4-maverick use a more formal structure, with clear headings and bullet points, while llama-4 uses a simpler format.\\n\\nRecommendation:\\n\\nBased on the evaluation, all three models are recommended for their ability to summarize the original text well and capture the key points. However, if a more detailed summary is required, llama-4-maverick may be the best choice. If a concise summary is preferred, llama-4 could be a good option. llama-3 provides a good balance between detail and conciseness.\", 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 2445, 'total_tokens': 3004, 'completion_tokens': 559, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the original text denoted by `email`\n",
    "and the name of several models: `model:<name of model>\n",
    "as well as the summary generated by that model: `summary`\n",
    "\n",
    "Provide an evaluation of each model's summary:\n",
    "- Does it summarize the original text well?\n",
    "- Does it follow the instructions of the prompt?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "email: ```{email}`\n",
    "\n",
    "model: llama-3\n",
    "summary: {response_7b}\n",
    "\n",
    "\n",
    "model: llama-4\n",
    "summary: {response_llama3_8b['choices'][0]['text']}\n",
    "\n",
    "model: llama-4-maverick\n",
    "summary: {response_llama3_70b['choices'][0]['text']}\n",
    "\"\"\"\n",
    "\n",
    "response_eval = llama(prompt,\n",
    "                model=\"meta-llama/llama-4-maverick-17b-128e-instruct-fp8\")\n",
    "print(response_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eaa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179a5581-bb28-4eba-be19-045be0779ed0",
   "metadata": {},
   "source": [
    "### Task 3: Reasoning ###\n",
    "- Compare the three models' performance on reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f98a66a-6d17-4a6d-bae8-881a5d5a1d4d",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Jeff and Tommy are neighbors\n",
    "\n",
    "Tommy and Eddy are not neighbors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5198c1-701d-4e07-93a1-38ab7a12e07c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Are Jeff and Eddy neighbors?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da3a23b9-0dde-4cc2-83cc-b9344b5983d4",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given this context: ```{context}```,\n",
    "\n",
    "and the following query:\n",
    "```{query}```\n",
    "\n",
    "Please answer the questions in the query and explain your reasoning.\n",
    "If there is not enough informaton to answer, please say\n",
    "\"I do not have enough information to answer this questions.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d63881-485c-4170-9bb0-a935ea79e4de",
   "metadata": {},
   "source": [
    "- First, use the 7B parameter chat model (`llama-2-7b-chat`) for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "544f3d8b-14e4-497b-9a3e-3bcc9f9f623c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The query asks if Jeff and Eddy are neighbors. To answer this, we need to consider the given context. We know that Jeff and Tommy are neighbors, but we don't have any information about Eddy. Therefore, we can't conclude whether Eddy is a neighbor of Jeff or not.\n",
      "\n",
      "So, the answer is:\n",
      "\"I do not have enough information to answer this question.\"\n"
     ]
    }
   ],
   "source": [
    "response_7b_chat = llama(prompt,\n",
    "                        model=\"meta-llama/Meta-Llama-3-8B-Instruct-Lite\")\n",
    "print(response_7b_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66b3dc-a373-4c30-8554-9ef855a076c2",
   "metadata": {},
   "source": [
    "- Now, use the 13B parameter chat model (`llama-2-13b-chat`) for the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7dc11-c961-4307-ba92-d0b88b9d8fc3",
   "metadata": {},
   "source": [
    "- Then, use the 70B parameter chat model (`llama-2-70b-chat`) for the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6f061-1040-42d5-81b1-e37709445c1e",
   "metadata": {},
   "source": [
    "- Lastly, use the Llama 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f958db46-abe8-4518-8db4-8829f00cde61",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'oPKGqpD-2kFHot-9ae665b8fa32e4e4-SEA', 'object': 'text_completion', 'created': 1765806496, 'model': 'meta-llama/Llama-4-Scout-17B-16E-Instruct', 'choices': [{'index': 0, 'text': '\\n\\nBased on the given context:\\n\\n1. Jeff and Tommy are neighbors.\\n2. Tommy and Eddy are not neighbors.\\n\\nWe can infer that just because Tommy and Eddy are not neighbors, and Jeff and Tommy are neighbors, it does not necessarily mean Jeff and Eddy are or are not neighbors. The context does not provide a direct relationship between Jeff and Eddy.\\n\\nTherefore, the answer to the query \"Are Jeff and Eddy neighbors?\" is:\\n\\n\"I do not have enough information to answer this question.\"\\n\\nThe given information does not establish a direct or indirect relationship between Jeff and Eddy that would allow us to conclusively determine if they are neighbors or not.', 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 200008}], 'usage': {'prompt_tokens': 77, 'total_tokens': 206, 'completion_tokens': 129, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}, 'prompt': []}\n"
     ]
    }
   ],
   "source": [
    "response_llama3_8b_chat = llama(prompt,\n",
    "                        model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "print(response_llama3_8b_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d7ea0b1-c040-4fb3-96c7-676e6f64442a",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " From the given context, we know that:\n",
      "\n",
      "1. Jeff and Tommy are neighbors.\n",
      "2. Tommy and Eddy are not neighbors.\n",
      "\n",
      "To determine if Jeff and Eddy are neighbors, let's analyze the information:\n",
      "\n",
      "- Since Jeff and Tommy are neighbors, Jeff lives either next to or very close to Tommy.\n",
      "- Tommy and Eddy are not neighbors, meaning Eddy does not live next to or very close to Tommy.\n",
      "\n",
      "Given that Tommy is between Jeff and Eddy (in the sense that Jeff is a neighbor of Tommy and Eddy is not), it doesn't directly tell us if Jeff and Eddy are neighbors because we don't know the exact arrangement of their houses. \n",
      "\n",
      "For example, if the houses are arranged linearly and Jeff is between Tommy and Eddy, or if Eddy is between Jeff and Tommy but not directly next to Tommy, Jeff and Eddy could potentially be neighbors. However, without knowing the specific arrangement or more details about their relative positions, we can't definitively conclude whether Jeff and Eddy are neighbors.\n",
      "\n",
      "Therefore, I do not have enough information to answer this question.\n"
     ]
    }
   ],
   "source": [
    "response_llama3_70b_chat = llama(prompt,\n",
    "                        model=\"META-LLAMA/Llama-4-Maverick-17B-128E-Instruct-FP8\")\n",
    "print(response_llama3_70b_chat['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017fdb2-3e5d-4f4b-862d-acdb3dc41e5c",
   "metadata": {},
   "source": [
    "#### Model-Graded Evaluation: Reasoning\n",
    "\n",
    "- Again, ask a LLM to compare the three responses.\n",
    "- Create a `prompt` that will evaluate these three responses using 70B parameter chat model (`llama-2-70b-chat`).\n",
    "- In the `prompt`, provide the `context`, `query`,\"name of the models\", and the \"response\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c742104-ef6d-43f6-afa0-6f8ae67ace4a",
   "metadata": {
    "height": 540
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the context `context:`,\n",
    "Also also given the query (the task): `query:`\n",
    "and given the name of several models: `mode:<name of model>,\n",
    "as well as the response generated by that model: `response:`\n",
    "\n",
    "Provide an evaluation of each model's response:\n",
    "- Does it answer the query accurately?\n",
    "- Does it provide a contradictory response?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "context: ```{context}```\n",
    "\n",
    "model: llama-2-7b-chat\n",
    "response: ```{response_7b_chat}```\n",
    "\n",
    "\n",
    "model: llama-4\n",
    "response: ```{response_llama3_8b_chat['choices'][0]['text']}```\n",
    "\n",
    "model: llama-4-maverick\n",
    "response: ``{response_llama3_70b_chat['choices'][0]['text']}``\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc6f1cde-68e2-4e67-96da-a98dd2e6bc45",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "### Evaluation of Model Responses\n",
      "\n",
      "#### Model: llama-2-7b-chat\n",
      "- **Accuracy**: The model accurately answers the query by stating that there is not enough information to determine if Jeff and Eddy are neighbors.\n",
      "- **Contradiction**: No, the response does not contradict the given context or the query.\n",
      "- **Other Characteristics**: The response is straightforward and clearly explains the reasoning behind the answer.\n",
      "\n",
      "#### Model: llama-4\n",
      "- **Accuracy**: The model accurately answers the query, concluding that there is insufficient information to determine the neighbor status between Jeff and Eddy.\n",
      "- **Contradiction**: No, the response is consistent with the context and query.\n",
      "- **Other Characteristics**: The response provides a detailed explanation of the thought process, emphasizing the lack of direct or indirect information about Jeff and Eddy's relationship.\n",
      "\n",
      "#### Model: llama-4-maverick\n",
      "- **Accuracy**: The model also accurately answers the query, stating that there is not enough information to conclude whether Jeff and Eddy are neighbors.\n",
      "- **Contradiction**: No, the response is in line with the given context and query.\n",
      "- **Other Characteristics**: This response provides a more elaborate analysis, considering possible arrangements of the houses and the implications of the given context. It demonstrates a deeper attempt to reason about the problem.\n",
      "\n",
      "### Comparison and Recommendation\n",
      "\n",
      "All three models (llama-2-7b-chat, llama-4, and llama-4-maverick) accurately answer the query and do not provide contradictory responses. They all conclude that there is insufficient information to determine if Jeff and Eddy are neighbors.\n",
      "\n",
      "- **llama-2-7b-chat** provides a clear and concise answer.\n",
      "- **llama-4** offers a detailed explanation, reinforcing the conclusion with a clear rationale.\n",
      "- **llama-4-maverick** goes a step further by exploring possible scenarios and house arrangements, demonstrating a more nuanced understanding of the problem.\n",
      "\n",
      "Based on the evaluation, **llama-4-maverick** stands out for its detailed analysis and attempt to reason about the possible implications of the given context. However, all models perform well in terms of accuracy and consistency. If the task requires a more in-depth analysis, **llama-4-maverick** is recommended. For straightforward and clear responses, **llama-2-7b-chat** or **llama-4** could be preferred, depending on whether a concise or a detailed explanation is valued more.\n"
     ]
    }
   ],
   "source": [
    "response_eval = llama(prompt, \n",
    "                      model=\"META-LLAMA/llama-4-maverick-17b-128e-instruct-fp8\")\n",
    "\n",
    "print(response_eval['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c20b2-599d-49ac-bca0-573360051a05",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
